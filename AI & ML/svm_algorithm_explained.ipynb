{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547b1401",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) Algorithm: A Deep Dive\n",
    "\n",
    "This notebook provides a comprehensive explanation of the SVM algorithm, including theory, types, kernel tricks, hyperparameters, and practical examples with Python code and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb7c360",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "We will use NumPy, pandas, matplotlib, seaborn, and scikit-learn for SVM demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d9a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaa7fa8",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset\n",
    "We will use the Iris dataset for binary classification (setosa vs. versicolor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f452f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset and select two classes for binary classification\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[iris.target != 2, :2]  # Use only first two features for visualization\n",
    "y = iris.target[iris.target != 2]\n",
    "\n",
    "# Create DataFrame for EDA\n",
    "df = pd.DataFrame(X, columns=iris.feature_names[:2])\n",
    "df['target'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "g = sns.scatterplot(x=iris.feature_names[0], y=iris.feature_names[1], hue='target', data=df, palette='Set1')\n",
    "g.set_title('Iris Dataset (Setosa vs. Versicolor)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd5fe9",
   "metadata": {},
   "source": [
    "## 3. Preprocess the Data\n",
    "We will split the data into training and test sets and standardize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d31c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and scale data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5c006",
   "metadata": {},
   "source": [
    "## 4. Train a Linear SVM\n",
    "Let's train a linear SVM and visualize the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear SVM\n",
    "svm_linear = SVC(kernel='linear')\n",
    "svm_linear.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = svm_linear.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f92bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary for linear SVM\n",
    "def plot_svm_decision_boundary(clf, X, y, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.2, cmap='coolwarm')\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, palette='Set1', s=60)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1 (standardized)')\n",
    "    plt.ylabel('Feature 2 (standardized)')\n",
    "    plt.legend(title='Class')\n",
    "    plt.show()\n",
    "\n",
    "plot_svm_decision_boundary(svm_linear, X_train_scaled, y_train, 'Linear SVM Decision Boundary (Train)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7a286",
   "metadata": {},
   "source": [
    "## 5. SVM with Nonlinear Kernel (RBF)\n",
    "Let's train an SVM with a radial basis function (RBF) kernel and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ba823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM with RBF kernel\n",
    "svm_rbf = SVC(kernel='rbf', gamma='auto')\n",
    "svm_rbf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred_rbf))\n",
    "\n",
    "plot_svm_decision_boundary(svm_rbf, X_train_scaled, y_train, 'RBF SVM Decision Boundary (Train)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4facc5",
   "metadata": {},
   "source": [
    "## 6. SVM Theory and Hyperparameters\n",
    "\n",
    "- **Support Vectors**: Data points closest to the decision boundary.\n",
    "- **Margin**: Distance between the decision boundary and the nearest data points.\n",
    "- **Kernel Trick**: Allows SVM to perform well on non-linearly separable data by mapping to higher dimensions.\n",
    "- **C (Regularization)**: Controls trade-off between maximizing margin and minimizing classification error.\n",
    "- **Gamma**: Defines influence of a single training example (for RBF/poly kernels).\n",
    "\n",
    "Try changing these parameters and observe the effect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7afb8a0",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "- SVMs are powerful for both linear and nonlinear classification.\n",
    "- The kernel trick enables SVMs to handle complex data.\n",
    "- Hyperparameters like C and gamma are crucial for model performance.\n",
    "\n",
    "Experiment with different kernels and parameters for deeper understanding!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
